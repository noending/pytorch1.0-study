{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Models\n",
    "\n",
    "In this notebook, I'll show you how to save and load models with PyTorch. This is important because you'll often want to load previously trained models to use in making predictions or to continue training on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper\n",
    "import fc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 233,
       "width": 233
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a network\n",
    "\n",
    "To make things more concise here, I moved the model architecture and training code from the last part to a file called `fc_model`. Importing this, we can easily create a fully-connected network with `fc_model.Network`, and train the network using `fc_model.train`. I'll use this model (once it's trained) to demonstrate how we can save and load models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "\n",
    "model = fc_model.Network(784, 10, [512, 256, 128])\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 1.713..  Test Loss: 0.988..  Test Accuracy: 0.638\n",
      "Epoch: 1/2..  Training Loss: 1.067..  Test Loss: 0.729..  Test Accuracy: 0.734\n",
      "Epoch: 1/2..  Training Loss: 0.866..  Test Loss: 0.691..  Test Accuracy: 0.741\n",
      "Epoch: 1/2..  Training Loss: 0.827..  Test Loss: 0.658..  Test Accuracy: 0.748\n",
      "Epoch: 1/2..  Training Loss: 0.714..  Test Loss: 0.636..  Test Accuracy: 0.759\n",
      "Epoch: 1/2..  Training Loss: 0.710..  Test Loss: 0.596..  Test Accuracy: 0.769\n",
      "Epoch: 1/2..  Training Loss: 0.722..  Test Loss: 0.613..  Test Accuracy: 0.765\n",
      "Epoch: 1/2..  Training Loss: 0.687..  Test Loss: 0.603..  Test Accuracy: 0.773\n",
      "Epoch: 1/2..  Training Loss: 0.706..  Test Loss: 0.584..  Test Accuracy: 0.778\n",
      "Epoch: 1/2..  Training Loss: 0.666..  Test Loss: 0.546..  Test Accuracy: 0.799\n",
      "Epoch: 1/2..  Training Loss: 0.644..  Test Loss: 0.536..  Test Accuracy: 0.796\n",
      "Epoch: 1/2..  Training Loss: 0.630..  Test Loss: 0.540..  Test Accuracy: 0.801\n",
      "Epoch: 1/2..  Training Loss: 0.630..  Test Loss: 0.522..  Test Accuracy: 0.803\n",
      "Epoch: 1/2..  Training Loss: 0.597..  Test Loss: 0.511..  Test Accuracy: 0.812\n",
      "Epoch: 1/2..  Training Loss: 0.569..  Test Loss: 0.537..  Test Accuracy: 0.804\n",
      "Epoch: 1/2..  Training Loss: 0.599..  Test Loss: 0.513..  Test Accuracy: 0.812\n",
      "Epoch: 1/2..  Training Loss: 0.553..  Test Loss: 0.496..  Test Accuracy: 0.825\n",
      "Epoch: 1/2..  Training Loss: 0.611..  Test Loss: 0.520..  Test Accuracy: 0.807\n",
      "Epoch: 1/2..  Training Loss: 0.568..  Test Loss: 0.497..  Test Accuracy: 0.817\n",
      "Epoch: 1/2..  Training Loss: 0.597..  Test Loss: 0.507..  Test Accuracy: 0.813\n",
      "Epoch: 1/2..  Training Loss: 0.548..  Test Loss: 0.510..  Test Accuracy: 0.806\n",
      "Epoch: 1/2..  Training Loss: 0.567..  Test Loss: 0.498..  Test Accuracy: 0.815\n",
      "Epoch: 1/2..  Training Loss: 0.568..  Test Loss: 0.477..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.549..  Test Loss: 0.477..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.537..  Test Loss: 0.474..  Test Accuracy: 0.828\n",
      "Epoch: 2/2..  Training Loss: 0.558..  Test Loss: 0.473..  Test Accuracy: 0.827\n",
      "Epoch: 2/2..  Training Loss: 0.560..  Test Loss: 0.475..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.533..  Test Loss: 0.485..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.528..  Test Loss: 0.460..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.499..  Test Loss: 0.459..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.518..  Test Loss: 0.449..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.539..  Test Loss: 0.460..  Test Accuracy: 0.829\n",
      "Epoch: 2/2..  Training Loss: 0.549..  Test Loss: 0.453..  Test Accuracy: 0.839\n",
      "Epoch: 2/2..  Training Loss: 0.570..  Test Loss: 0.455..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.531..  Test Loss: 0.443..  Test Accuracy: 0.838\n",
      "Epoch: 2/2..  Training Loss: 0.523..  Test Loss: 0.455..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.528..  Test Loss: 0.450..  Test Accuracy: 0.837\n",
      "Epoch: 2/2..  Training Loss: 0.537..  Test Loss: 0.453..  Test Accuracy: 0.834\n",
      "Epoch: 2/2..  Training Loss: 0.516..  Test Loss: 0.448..  Test Accuracy: 0.840\n",
      "Epoch: 2/2..  Training Loss: 0.506..  Test Loss: 0.453..  Test Accuracy: 0.833\n",
      "Epoch: 2/2..  Training Loss: 0.553..  Test Loss: 0.441..  Test Accuracy: 0.841\n",
      "Epoch: 2/2..  Training Loss: 0.496..  Test Loss: 0.456..  Test Accuracy: 0.837\n",
      "Epoch: 2/2..  Training Loss: 0.506..  Test Loss: 0.445..  Test Accuracy: 0.836\n",
      "Epoch: 2/2..  Training Loss: 0.506..  Test Loss: 0.456..  Test Accuracy: 0.833\n",
      "Epoch: 2/2..  Training Loss: 0.533..  Test Loss: 0.442..  Test Accuracy: 0.838\n",
      "Epoch: 2/2..  Training Loss: 0.511..  Test Loss: 0.455..  Test Accuracy: 0.837\n"
     ]
    }
   ],
   "source": [
    "fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading networks\n",
    "\n",
    "As you can imagine, it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions.\n",
    "\n",
    "The parameters for PyTorch networks are stored in a model's `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model: \n",
      "\n",
      " Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ") \n",
      "\n",
      "The state dict keys: \n",
      "\n",
      " odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights are: \n",
      " OrderedDict([('hidden_layers.0.weight', tensor([[-0.0128, -0.0145,  0.0211,  ..., -0.0237, -0.0109, -0.0196],\n",
      "        [-0.0225, -0.0082,  0.0232,  ...,  0.0211, -0.0333,  0.0061],\n",
      "        [ 0.0096, -0.0150,  0.0001,  ..., -0.0018,  0.0092, -0.0057],\n",
      "        ...,\n",
      "        [ 0.0049, -0.0132, -0.0111,  ..., -0.0340,  0.0097, -0.0279],\n",
      "        [-0.0232, -0.0312,  0.0086,  ..., -0.0185, -0.0044, -0.0034],\n",
      "        [ 0.0074, -0.0279,  0.0080,  ..., -0.0171,  0.0071,  0.0168]])), ('hidden_layers.0.bias', tensor([-1.2291e-02, -2.9585e-03, -9.3758e-03,  5.3945e-03,  2.0006e-02,\n",
      "        -1.5874e-03, -2.2516e-02,  6.1070e-03, -5.0613e-03,  2.3498e-02,\n",
      "        -2.2980e-02, -1.4966e-02,  3.5076e-02,  6.3029e-03, -1.0960e-02,\n",
      "        -6.7093e-03, -1.3753e-02,  2.8525e-02, -2.5703e-02,  3.3555e-02,\n",
      "         1.0541e-02, -3.4635e-02, -1.1230e-02, -1.6710e-02, -2.2678e-02,\n",
      "        -7.6035e-04, -9.7222e-04,  3.5436e-02, -3.5341e-02, -9.6542e-03,\n",
      "         2.6481e-02,  2.3000e-02, -2.8301e-02, -1.7989e-02,  8.4057e-03,\n",
      "        -1.0148e-02,  2.6026e-02,  2.8403e-02, -8.4050e-03,  1.9267e-02,\n",
      "        -6.1202e-03, -7.1118e-03, -9.5352e-03,  2.4190e-02, -8.4642e-03,\n",
      "         3.0712e-02,  2.6647e-02,  2.1157e-02, -2.7930e-03, -1.6353e-02,\n",
      "        -1.3713e-02,  7.1887e-03,  4.1492e-03,  2.5648e-02,  1.8014e-03,\n",
      "        -3.1496e-02,  2.1848e-02,  3.1186e-02,  1.8113e-02, -7.8903e-03,\n",
      "         2.8782e-02, -4.8925e-03, -2.8165e-02,  1.4294e-03,  1.0814e-03,\n",
      "         8.6733e-03, -9.5397e-03, -3.0903e-03,  3.0722e-02,  3.4867e-02,\n",
      "         5.6962e-03, -1.7101e-02, -2.4261e-02,  3.1647e-02,  1.4386e-02,\n",
      "        -2.4122e-02, -1.2587e-02,  2.7397e-03,  2.7381e-02, -3.5240e-02,\n",
      "        -4.7257e-03,  3.0567e-02,  2.2141e-02,  3.0767e-02, -3.1105e-02,\n",
      "         3.3621e-02, -3.1371e-02,  8.2999e-03,  2.7356e-02,  1.6168e-02,\n",
      "         1.8954e-02,  2.6301e-02,  2.7884e-03,  3.0709e-02, -2.2577e-02,\n",
      "        -1.2901e-04, -1.5619e-02, -2.3403e-02,  8.1838e-03, -5.4016e-03,\n",
      "         1.7166e-02, -1.5280e-02,  3.4083e-03, -1.9342e-02,  1.4468e-02,\n",
      "         1.7600e-03,  2.3096e-03, -6.8564e-03,  1.5048e-02,  2.5854e-02,\n",
      "         2.4456e-02, -1.6153e-02, -3.4499e-02, -1.2469e-02,  5.8746e-03,\n",
      "        -4.8781e-03, -3.5472e-02,  2.2625e-02, -1.7206e-02,  1.5427e-02,\n",
      "        -7.4713e-03, -1.4504e-02, -1.2262e-02,  1.9323e-02, -3.0036e-02,\n",
      "        -2.9307e-02,  2.3351e-02,  5.9084e-03,  3.4527e-02, -1.0191e-02,\n",
      "        -8.9753e-03,  3.4031e-02, -3.0903e-02,  3.3827e-02, -1.5629e-02,\n",
      "        -1.0642e-02,  2.7221e-02,  4.0195e-03, -5.8975e-05,  3.0067e-02,\n",
      "        -4.3176e-03,  1.9348e-03,  7.8818e-03, -7.8695e-03, -3.1763e-02,\n",
      "         1.5891e-02,  1.1523e-02, -2.6195e-02, -9.6959e-03,  1.5378e-02,\n",
      "         3.3091e-02,  1.9505e-02, -2.1003e-02, -3.3922e-02,  3.0143e-02,\n",
      "         2.4693e-02,  1.9808e-02,  1.5288e-02, -7.2705e-03,  2.3635e-02,\n",
      "         9.2830e-03,  2.2189e-02, -2.5669e-02, -1.6692e-02, -1.8052e-02,\n",
      "        -6.4246e-03, -1.3745e-02, -2.7999e-02, -2.4485e-02,  2.0090e-02,\n",
      "         2.8162e-02, -2.1993e-02,  5.7848e-03, -3.4616e-02,  2.3716e-02,\n",
      "         2.9048e-02,  3.4648e-02,  2.8366e-02, -2.6920e-02, -2.8381e-02,\n",
      "         3.1536e-02,  7.4704e-03, -2.1001e-02, -3.4360e-02, -1.4917e-02,\n",
      "         1.2098e-02,  1.5040e-02,  2.8579e-02, -3.2273e-02, -1.0582e-02,\n",
      "        -3.6166e-03,  2.6867e-02, -5.5322e-03, -3.3236e-02, -2.6939e-02,\n",
      "         3.0848e-02,  1.8275e-02,  6.2977e-03,  2.5780e-02, -2.8707e-02,\n",
      "         2.6395e-02,  2.0692e-02, -1.5404e-02, -7.3664e-03, -1.3441e-02,\n",
      "         2.2496e-02,  1.5835e-03, -2.5076e-02,  1.7621e-02,  1.2411e-02,\n",
      "         6.4663e-03,  3.0023e-04, -1.0900e-02, -1.5620e-03,  7.3923e-03,\n",
      "         7.9443e-03, -1.8339e-02, -1.7439e-02, -1.5296e-02, -2.3120e-02,\n",
      "        -2.5471e-02,  6.3829e-03, -3.3428e-03, -3.1111e-02, -3.0768e-02,\n",
      "         1.7778e-02,  3.9883e-04, -3.0917e-02, -1.3717e-02, -1.3244e-02,\n",
      "        -3.3958e-02, -2.4978e-03, -1.2068e-02,  9.7595e-03,  5.9671e-04,\n",
      "        -1.3523e-03, -2.7034e-02, -3.0958e-02,  2.6084e-03,  7.9253e-04,\n",
      "        -2.1577e-02, -1.4546e-03, -2.6417e-02, -1.8296e-02, -2.3142e-02,\n",
      "         1.4414e-03,  2.6037e-02, -2.1510e-02, -2.4240e-02, -4.1934e-03,\n",
      "        -3.2809e-02,  3.2521e-02, -1.6037e-02, -4.5954e-03, -2.2153e-02,\n",
      "         3.2685e-02,  3.5024e-03, -3.0274e-02, -2.8588e-02, -2.5819e-02,\n",
      "         2.2891e-02,  1.3786e-02, -1.6041e-02,  1.6798e-02,  3.3287e-02,\n",
      "        -2.2744e-02,  1.2555e-02,  2.8398e-02, -1.4017e-02, -3.5132e-02,\n",
      "         9.4592e-03,  2.9882e-02,  1.2918e-02,  2.3449e-02, -2.2558e-02,\n",
      "        -1.7411e-02, -1.0852e-02, -1.2234e-02, -3.0698e-02,  3.2554e-02,\n",
      "        -1.7016e-02, -2.9759e-02,  7.5856e-03, -2.8870e-02, -2.9829e-02,\n",
      "        -1.0852e-02, -7.5544e-03,  1.6580e-02, -2.2658e-02,  2.8160e-02,\n",
      "         3.3152e-02,  2.1720e-02, -1.2452e-02, -2.3998e-02,  5.7841e-03,\n",
      "        -7.3342e-03, -2.7532e-02, -5.6250e-03,  1.2687e-03, -8.7201e-03,\n",
      "         7.9854e-03, -4.0543e-03, -3.0166e-02, -2.9713e-02,  2.1477e-02,\n",
      "        -2.6397e-02,  1.0272e-02,  2.6648e-02, -3.4819e-02,  2.4676e-02,\n",
      "        -2.8256e-02,  8.1320e-03,  2.2369e-02, -2.8153e-02,  2.9092e-02,\n",
      "         3.1148e-02,  1.9676e-02,  8.8892e-03, -4.9456e-03,  5.9151e-03,\n",
      "        -1.0674e-02, -1.4215e-02, -1.5214e-02,  2.3177e-02, -2.4468e-02,\n",
      "        -2.4361e-02,  2.5894e-02,  3.5333e-02,  3.1137e-02, -2.9554e-02,\n",
      "         2.8947e-02,  3.5437e-02, -2.2017e-02, -2.2327e-03, -1.9309e-03,\n",
      "        -1.0620e-02,  1.5190e-02, -3.2402e-02, -1.7714e-02,  3.6669e-03,\n",
      "         1.0836e-02, -1.6914e-02,  2.6429e-02, -3.5654e-02,  2.4008e-02,\n",
      "        -1.1179e-02, -1.0053e-02,  2.9937e-03,  1.5902e-02, -3.2040e-02,\n",
      "         1.0696e-02,  2.7716e-02,  1.6489e-02,  3.1840e-02,  1.2731e-02,\n",
      "         7.2149e-03, -8.4833e-04,  1.5450e-02,  6.4492e-03,  3.1217e-02,\n",
      "         2.7433e-03,  1.9806e-02, -1.1541e-02, -1.4114e-02, -1.4440e-02,\n",
      "         3.4628e-02,  1.0990e-02, -3.4105e-02,  9.5124e-03,  8.5445e-03,\n",
      "        -1.6453e-02,  5.3161e-03,  2.6167e-02, -3.2698e-03,  2.3324e-02,\n",
      "        -8.4476e-03,  2.9766e-02,  1.3005e-02,  1.1357e-02, -9.4052e-03,\n",
      "         1.8536e-02,  1.9751e-02, -2.1582e-02, -2.5675e-02,  7.7120e-03,\n",
      "         1.1268e-02, -5.7343e-03,  3.4173e-02, -2.2724e-02, -2.5981e-02,\n",
      "        -2.7126e-02,  5.4744e-03,  5.8549e-03, -1.5061e-02,  1.2360e-02,\n",
      "        -2.3837e-02,  4.4748e-03,  1.1448e-02, -3.3212e-02, -2.2510e-02,\n",
      "        -7.7091e-03,  1.5457e-02,  2.6845e-02,  3.0780e-02,  1.0686e-02,\n",
      "        -7.5512e-03,  1.0926e-02,  1.0185e-02,  2.3075e-02, -2.0998e-02,\n",
      "        -2.2419e-03,  3.1275e-02, -2.0239e-02, -3.2225e-02, -7.7744e-03,\n",
      "        -2.8569e-02, -2.3795e-02,  7.9422e-03,  1.5573e-02,  1.6094e-02,\n",
      "         2.3121e-02,  2.2622e-03,  3.3254e-02, -1.7701e-02,  3.3048e-02,\n",
      "         4.7434e-03, -2.3308e-02,  1.2761e-02,  2.2578e-02,  8.1697e-03,\n",
      "         3.2488e-02, -1.4124e-02, -2.8213e-02, -3.3251e-02,  7.1513e-03,\n",
      "         1.2034e-02, -9.3064e-03, -2.5675e-02,  3.2031e-03,  1.7520e-02,\n",
      "         2.0057e-02, -3.4198e-02,  5.2604e-03, -2.3294e-02,  1.0786e-02,\n",
      "         1.9523e-02, -3.3382e-02, -1.9586e-02, -1.7258e-02,  1.1517e-02,\n",
      "         2.7054e-03,  2.5431e-02, -1.0786e-02, -1.1577e-02,  7.5847e-03,\n",
      "         1.3339e-02,  1.2191e-02,  2.3007e-02,  1.7244e-02, -5.1471e-03,\n",
      "         3.1722e-02, -1.1766e-02, -1.9214e-02,  3.4409e-02,  2.5964e-02,\n",
      "        -2.9248e-02, -3.3239e-02, -9.9458e-03, -3.3175e-02,  6.9036e-03,\n",
      "         2.1865e-02,  2.8546e-02, -5.2925e-03,  1.2753e-02, -1.2399e-02,\n",
      "        -1.1003e-02,  8.9565e-04, -3.3262e-02, -7.6586e-03,  3.3485e-02,\n",
      "         6.6119e-03, -3.3369e-02, -1.2091e-02, -2.9916e-02,  9.6755e-03,\n",
      "         1.1942e-02, -1.9451e-02, -1.8380e-03, -2.8469e-02,  2.9745e-02,\n",
      "         7.3945e-03,  1.7750e-02, -8.0584e-03,  2.7539e-02,  2.0589e-02,\n",
      "        -1.8220e-02, -3.4796e-02,  6.6358e-03,  9.1366e-03,  2.6362e-02,\n",
      "        -1.3130e-02, -2.9548e-02,  1.1143e-02,  1.8537e-02,  3.4009e-02,\n",
      "         4.8852e-03, -1.1809e-02, -8.7837e-03, -1.8480e-02, -2.0438e-03,\n",
      "        -2.1678e-02, -3.1268e-02])), ('hidden_layers.1.weight', tensor([[ 0.0190,  0.0162,  0.0154,  ...,  0.0439,  0.0238,  0.0040],\n",
      "        [ 0.0298,  0.0236, -0.0216,  ...,  0.0178,  0.0287,  0.0190],\n",
      "        [-0.0071, -0.0179, -0.0118,  ...,  0.0091, -0.0283, -0.0402],\n",
      "        ...,\n",
      "        [-0.0089,  0.0183,  0.0103,  ...,  0.0418,  0.0383, -0.0394],\n",
      "        [ 0.0243, -0.0208, -0.0064,  ...,  0.0435, -0.0043,  0.0200],\n",
      "        [-0.0013,  0.0100,  0.0407,  ..., -0.0150, -0.0407, -0.0086]])), ('hidden_layers.1.bias', tensor([-0.0178, -0.0406,  0.0163,  0.0306, -0.0336, -0.0122,  0.0362, -0.0070,\n",
      "         0.0429, -0.0400, -0.0132,  0.0384,  0.0302, -0.0295, -0.0130, -0.0254,\n",
      "        -0.0115, -0.0339,  0.0430, -0.0324,  0.0152, -0.0412, -0.0342, -0.0022,\n",
      "        -0.0293,  0.0255,  0.0027, -0.0053,  0.0027, -0.0010,  0.0092,  0.0367,\n",
      "         0.0101,  0.0083, -0.0047,  0.0163,  0.0350,  0.0043, -0.0355,  0.0374,\n",
      "        -0.0147,  0.0114, -0.0239, -0.0016, -0.0425, -0.0400, -0.0150,  0.0120,\n",
      "        -0.0273, -0.0043, -0.0317,  0.0437,  0.0351, -0.0402, -0.0192, -0.0179,\n",
      "         0.0363, -0.0074, -0.0043,  0.0115,  0.0367,  0.0057,  0.0041,  0.0169,\n",
      "        -0.0284, -0.0160,  0.0193, -0.0361, -0.0399,  0.0023, -0.0031,  0.0358,\n",
      "        -0.0081, -0.0380, -0.0314, -0.0036, -0.0040,  0.0145, -0.0005,  0.0080,\n",
      "        -0.0376, -0.0348,  0.0127, -0.0190,  0.0040,  0.0041, -0.0024,  0.0429,\n",
      "        -0.0405, -0.0182, -0.0075, -0.0375, -0.0354, -0.0003,  0.0101, -0.0244,\n",
      "        -0.0152, -0.0070,  0.0341,  0.0185, -0.0171,  0.0245,  0.0139,  0.0398,\n",
      "         0.0442, -0.0361,  0.0056,  0.0436, -0.0218, -0.0322,  0.0345, -0.0097,\n",
      "        -0.0305, -0.0121, -0.0311,  0.0070, -0.0379,  0.0190, -0.0380,  0.0388,\n",
      "         0.0338,  0.0376,  0.0285,  0.0376, -0.0154, -0.0433, -0.0183, -0.0022,\n",
      "         0.0346, -0.0144,  0.0199, -0.0285, -0.0275, -0.0262, -0.0200,  0.0118,\n",
      "        -0.0142, -0.0154, -0.0322,  0.0287,  0.0089,  0.0363, -0.0415, -0.0156,\n",
      "         0.0305,  0.0284,  0.0303,  0.0205, -0.0138,  0.0277,  0.0062,  0.0357,\n",
      "        -0.0120,  0.0248, -0.0023,  0.0116, -0.0393,  0.0223, -0.0244,  0.0052,\n",
      "        -0.0086, -0.0305,  0.0107,  0.0356, -0.0089,  0.0025,  0.0278, -0.0057,\n",
      "         0.0122, -0.0242, -0.0415,  0.0386,  0.0094, -0.0322,  0.0428,  0.0113,\n",
      "         0.0019, -0.0076,  0.0420,  0.0003, -0.0337, -0.0076,  0.0319, -0.0325,\n",
      "        -0.0162,  0.0137, -0.0138,  0.0416,  0.0213,  0.0294, -0.0138,  0.0144,\n",
      "         0.0375, -0.0315, -0.0426, -0.0298, -0.0411,  0.0244, -0.0249, -0.0025,\n",
      "         0.0426,  0.0141,  0.0285, -0.0400,  0.0006, -0.0180,  0.0423, -0.0428,\n",
      "        -0.0388, -0.0208, -0.0212, -0.0191, -0.0139, -0.0013, -0.0432, -0.0179,\n",
      "        -0.0023,  0.0028,  0.0121,  0.0280,  0.0376,  0.0384,  0.0410,  0.0231,\n",
      "         0.0015, -0.0139, -0.0045,  0.0071,  0.0205,  0.0146, -0.0141,  0.0054,\n",
      "         0.0363, -0.0032,  0.0228,  0.0065,  0.0175,  0.0119,  0.0101,  0.0012,\n",
      "         0.0169,  0.0205,  0.0115,  0.0038,  0.0018, -0.0061,  0.0101, -0.0344,\n",
      "         0.0116,  0.0320,  0.0142, -0.0095,  0.0283,  0.0124,  0.0078, -0.0172])), ('hidden_layers.2.weight', tensor([[-0.0493,  0.0602, -0.0208,  ..., -0.0082,  0.0241, -0.0175],\n",
      "        [ 0.0310, -0.0265,  0.0285,  ..., -0.0444,  0.0420,  0.0474],\n",
      "        [-0.0246, -0.0610, -0.0390,  ..., -0.0615, -0.0143, -0.0507],\n",
      "        ...,\n",
      "        [-0.0110,  0.0442,  0.0564,  ...,  0.0169,  0.0473, -0.0275],\n",
      "        [-0.0139, -0.0318, -0.0362,  ..., -0.0225, -0.0255,  0.0317],\n",
      "        [-0.0411,  0.0186, -0.0009,  ...,  0.0543, -0.0085,  0.0215]])), ('hidden_layers.2.bias', tensor([ 0.0027,  0.0535, -0.0124, -0.0091,  0.0225,  0.0404,  0.0289,  0.0420,\n",
      "        -0.0597, -0.0107,  0.0247,  0.0562,  0.0441, -0.0159,  0.0620, -0.0448,\n",
      "         0.0048,  0.0208, -0.0517,  0.0405, -0.0126, -0.0459, -0.0599, -0.0206,\n",
      "        -0.0254,  0.0293, -0.0608, -0.0255, -0.0280,  0.0068, -0.0346,  0.0520,\n",
      "        -0.0136, -0.0579, -0.0526, -0.0208,  0.0319, -0.0119, -0.0570, -0.0227,\n",
      "         0.0255, -0.0537, -0.0109,  0.0224, -0.0367, -0.0377, -0.0127,  0.0155,\n",
      "         0.0225,  0.0132, -0.0427,  0.0289, -0.0120,  0.0513, -0.0336, -0.0453,\n",
      "        -0.0198,  0.0464,  0.0282, -0.0463, -0.0393, -0.0216, -0.0354, -0.0466,\n",
      "         0.0295, -0.0262, -0.0345, -0.0319,  0.0422,  0.0155,  0.0532, -0.0092,\n",
      "         0.0379, -0.0485, -0.0559,  0.0533, -0.0357,  0.0612,  0.0357,  0.0381,\n",
      "        -0.0364,  0.0122, -0.0035, -0.0201,  0.0438, -0.0246, -0.0299,  0.0376,\n",
      "        -0.0356, -0.0507,  0.0433,  0.0295, -0.0335,  0.0295,  0.0073, -0.0392,\n",
      "        -0.0481,  0.0082, -0.0278,  0.0463,  0.0548, -0.0223,  0.0196,  0.0572,\n",
      "        -0.0138,  0.0139, -0.0512, -0.0276, -0.0301, -0.0325,  0.0418, -0.0296,\n",
      "         0.0461, -0.0161, -0.0418,  0.0087, -0.0074, -0.0586, -0.0354, -0.0533,\n",
      "         0.0031, -0.0582,  0.0275, -0.0027, -0.0478, -0.0128,  0.0260, -0.0418])), ('output.weight', tensor([[-0.0856, -0.0111,  0.0422,  ..., -0.0654,  0.0772, -0.0054],\n",
      "        [ 0.0120, -0.0290,  0.0502,  ..., -0.0491, -0.0262,  0.0850],\n",
      "        [-0.0589,  0.0503, -0.0683,  ..., -0.0519, -0.0192,  0.0229],\n",
      "        ...,\n",
      "        [-0.0450, -0.0597, -0.0258,  ..., -0.0042,  0.0730,  0.0778],\n",
      "        [ 0.0671,  0.0382,  0.0026,  ..., -0.0182, -0.0384, -0.0558],\n",
      "        [-0.0757, -0.0159,  0.0802,  ..., -0.0659, -0.0601, -0.0589]])), ('output.bias', tensor([ 0.0001, -0.0868, -0.0847,  0.0356, -0.0327, -0.0376, -0.0620,  0.0151,\n",
      "         0.0838,  0.0427]))])\n"
     ]
    }
   ],
   "source": [
    "print(\"weights are: \\n\",model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest thing to do is simply save the state dict with `torch.save`. For example, we can save it to a file `'checkpoint.pth'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can load the state dict with `torch.load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to load the state dict in to the network, you do `model.load_state_dict(state_dict)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty straightforward, but as usual it's a bit more complicated. Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. If I create a model with a different architecture, this fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([129, 256]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([129]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 129]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b9c277cbdab0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m129\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# This will throw an error because the tensor sizes are wrong!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 769\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_named_members\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([129, 256]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([129]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 129])."
     ]
    }
   ],
   "source": [
    "# Try this\n",
    "model = fc_model.Network(784, 10, [512, 256, 129])\n",
    "# This will throw an error because the tensor sizes are wrong!\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved in the checkpoint, along with the state dict. To do this, you build a dictionary with all the information you need to compeletely rebuild the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the checkpoint has all the necessary information to rebuild the trained model. You can easily make that a function if you want. Similarly, we can write a function to load checkpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = fc_model.Network(checkpoint['input_size'],\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=129, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=129, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint('checkpoint.pth')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.load_state_dict(state_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 权重保存在state_dict()中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('hidden_layers.0.weight', tensor([[-0.0128, -0.0145,  0.0211,  ..., -0.0237, -0.0109, -0.0196],\n",
      "        [-0.0225, -0.0082,  0.0232,  ...,  0.0211, -0.0333,  0.0061],\n",
      "        [ 0.0096, -0.0150,  0.0001,  ..., -0.0018,  0.0092, -0.0057],\n",
      "        ...,\n",
      "        [ 0.0049, -0.0132, -0.0111,  ..., -0.0340,  0.0097, -0.0279],\n",
      "        [-0.0232, -0.0312,  0.0086,  ..., -0.0185, -0.0044, -0.0034],\n",
      "        [ 0.0074, -0.0279,  0.0080,  ..., -0.0171,  0.0071,  0.0168]])), ('hidden_layers.0.bias', tensor([-1.2291e-02, -2.9585e-03, -9.3758e-03,  5.3945e-03,  2.0006e-02,\n",
      "        -1.5874e-03, -2.2516e-02,  6.1070e-03, -5.0613e-03,  2.3498e-02,\n",
      "        -2.2980e-02, -1.4966e-02,  3.5076e-02,  6.3029e-03, -1.0960e-02,\n",
      "        -6.7093e-03, -1.3753e-02,  2.8525e-02, -2.5703e-02,  3.3555e-02,\n",
      "         1.0541e-02, -3.4635e-02, -1.1230e-02, -1.6710e-02, -2.2678e-02,\n",
      "        -7.6035e-04, -9.7222e-04,  3.5436e-02, -3.5341e-02, -9.6542e-03,\n",
      "         2.6481e-02,  2.3000e-02, -2.8301e-02, -1.7989e-02,  8.4057e-03,\n",
      "        -1.0148e-02,  2.6026e-02,  2.8403e-02, -8.4050e-03,  1.9267e-02,\n",
      "        -6.1202e-03, -7.1118e-03, -9.5352e-03,  2.4190e-02, -8.4642e-03,\n",
      "         3.0712e-02,  2.6647e-02,  2.1157e-02, -2.7930e-03, -1.6353e-02,\n",
      "        -1.3713e-02,  7.1887e-03,  4.1492e-03,  2.5648e-02,  1.8014e-03,\n",
      "        -3.1496e-02,  2.1848e-02,  3.1186e-02,  1.8113e-02, -7.8903e-03,\n",
      "         2.8782e-02, -4.8925e-03, -2.8165e-02,  1.4294e-03,  1.0814e-03,\n",
      "         8.6733e-03, -9.5397e-03, -3.0903e-03,  3.0722e-02,  3.4867e-02,\n",
      "         5.6962e-03, -1.7101e-02, -2.4261e-02,  3.1647e-02,  1.4386e-02,\n",
      "        -2.4122e-02, -1.2587e-02,  2.7397e-03,  2.7381e-02, -3.5240e-02,\n",
      "        -4.7257e-03,  3.0567e-02,  2.2141e-02,  3.0767e-02, -3.1105e-02,\n",
      "         3.3621e-02, -3.1371e-02,  8.2999e-03,  2.7356e-02,  1.6168e-02,\n",
      "         1.8954e-02,  2.6301e-02,  2.7884e-03,  3.0709e-02, -2.2577e-02,\n",
      "        -1.2901e-04, -1.5619e-02, -2.3403e-02,  8.1838e-03, -5.4016e-03,\n",
      "         1.7166e-02, -1.5280e-02,  3.4083e-03, -1.9342e-02,  1.4468e-02,\n",
      "         1.7600e-03,  2.3096e-03, -6.8564e-03,  1.5048e-02,  2.5854e-02,\n",
      "         2.4456e-02, -1.6153e-02, -3.4499e-02, -1.2469e-02,  5.8746e-03,\n",
      "        -4.8781e-03, -3.5472e-02,  2.2625e-02, -1.7206e-02,  1.5427e-02,\n",
      "        -7.4713e-03, -1.4504e-02, -1.2262e-02,  1.9323e-02, -3.0036e-02,\n",
      "        -2.9307e-02,  2.3351e-02,  5.9084e-03,  3.4527e-02, -1.0191e-02,\n",
      "        -8.9753e-03,  3.4031e-02, -3.0903e-02,  3.3827e-02, -1.5629e-02,\n",
      "        -1.0642e-02,  2.7221e-02,  4.0195e-03, -5.8975e-05,  3.0067e-02,\n",
      "        -4.3176e-03,  1.9348e-03,  7.8818e-03, -7.8695e-03, -3.1763e-02,\n",
      "         1.5891e-02,  1.1523e-02, -2.6195e-02, -9.6959e-03,  1.5378e-02,\n",
      "         3.3091e-02,  1.9505e-02, -2.1003e-02, -3.3922e-02,  3.0143e-02,\n",
      "         2.4693e-02,  1.9808e-02,  1.5288e-02, -7.2705e-03,  2.3635e-02,\n",
      "         9.2830e-03,  2.2189e-02, -2.5669e-02, -1.6692e-02, -1.8052e-02,\n",
      "        -6.4246e-03, -1.3745e-02, -2.7999e-02, -2.4485e-02,  2.0090e-02,\n",
      "         2.8162e-02, -2.1993e-02,  5.7848e-03, -3.4616e-02,  2.3716e-02,\n",
      "         2.9048e-02,  3.4648e-02,  2.8366e-02, -2.6920e-02, -2.8381e-02,\n",
      "         3.1536e-02,  7.4704e-03, -2.1001e-02, -3.4360e-02, -1.4917e-02,\n",
      "         1.2098e-02,  1.5040e-02,  2.8579e-02, -3.2273e-02, -1.0582e-02,\n",
      "        -3.6166e-03,  2.6867e-02, -5.5322e-03, -3.3236e-02, -2.6939e-02,\n",
      "         3.0848e-02,  1.8275e-02,  6.2977e-03,  2.5780e-02, -2.8707e-02,\n",
      "         2.6395e-02,  2.0692e-02, -1.5404e-02, -7.3664e-03, -1.3441e-02,\n",
      "         2.2496e-02,  1.5835e-03, -2.5076e-02,  1.7621e-02,  1.2411e-02,\n",
      "         6.4663e-03,  3.0023e-04, -1.0900e-02, -1.5620e-03,  7.3923e-03,\n",
      "         7.9443e-03, -1.8339e-02, -1.7439e-02, -1.5296e-02, -2.3120e-02,\n",
      "        -2.5471e-02,  6.3829e-03, -3.3428e-03, -3.1111e-02, -3.0768e-02,\n",
      "         1.7778e-02,  3.9883e-04, -3.0917e-02, -1.3717e-02, -1.3244e-02,\n",
      "        -3.3958e-02, -2.4978e-03, -1.2068e-02,  9.7595e-03,  5.9671e-04,\n",
      "        -1.3523e-03, -2.7034e-02, -3.0958e-02,  2.6084e-03,  7.9253e-04,\n",
      "        -2.1577e-02, -1.4546e-03, -2.6417e-02, -1.8296e-02, -2.3142e-02,\n",
      "         1.4414e-03,  2.6037e-02, -2.1510e-02, -2.4240e-02, -4.1934e-03,\n",
      "        -3.2809e-02,  3.2521e-02, -1.6037e-02, -4.5954e-03, -2.2153e-02,\n",
      "         3.2685e-02,  3.5024e-03, -3.0274e-02, -2.8588e-02, -2.5819e-02,\n",
      "         2.2891e-02,  1.3786e-02, -1.6041e-02,  1.6798e-02,  3.3287e-02,\n",
      "        -2.2744e-02,  1.2555e-02,  2.8398e-02, -1.4017e-02, -3.5132e-02,\n",
      "         9.4592e-03,  2.9882e-02,  1.2918e-02,  2.3449e-02, -2.2558e-02,\n",
      "        -1.7411e-02, -1.0852e-02, -1.2234e-02, -3.0698e-02,  3.2554e-02,\n",
      "        -1.7016e-02, -2.9759e-02,  7.5856e-03, -2.8870e-02, -2.9829e-02,\n",
      "        -1.0852e-02, -7.5544e-03,  1.6580e-02, -2.2658e-02,  2.8160e-02,\n",
      "         3.3152e-02,  2.1720e-02, -1.2452e-02, -2.3998e-02,  5.7841e-03,\n",
      "        -7.3342e-03, -2.7532e-02, -5.6250e-03,  1.2687e-03, -8.7201e-03,\n",
      "         7.9854e-03, -4.0543e-03, -3.0166e-02, -2.9713e-02,  2.1477e-02,\n",
      "        -2.6397e-02,  1.0272e-02,  2.6648e-02, -3.4819e-02,  2.4676e-02,\n",
      "        -2.8256e-02,  8.1320e-03,  2.2369e-02, -2.8153e-02,  2.9092e-02,\n",
      "         3.1148e-02,  1.9676e-02,  8.8892e-03, -4.9456e-03,  5.9151e-03,\n",
      "        -1.0674e-02, -1.4215e-02, -1.5214e-02,  2.3177e-02, -2.4468e-02,\n",
      "        -2.4361e-02,  2.5894e-02,  3.5333e-02,  3.1137e-02, -2.9554e-02,\n",
      "         2.8947e-02,  3.5437e-02, -2.2017e-02, -2.2327e-03, -1.9309e-03,\n",
      "        -1.0620e-02,  1.5190e-02, -3.2402e-02, -1.7714e-02,  3.6669e-03,\n",
      "         1.0836e-02, -1.6914e-02,  2.6429e-02, -3.5654e-02,  2.4008e-02,\n",
      "        -1.1179e-02, -1.0053e-02,  2.9937e-03,  1.5902e-02, -3.2040e-02,\n",
      "         1.0696e-02,  2.7716e-02,  1.6489e-02,  3.1840e-02,  1.2731e-02,\n",
      "         7.2149e-03, -8.4833e-04,  1.5450e-02,  6.4492e-03,  3.1217e-02,\n",
      "         2.7433e-03,  1.9806e-02, -1.1541e-02, -1.4114e-02, -1.4440e-02,\n",
      "         3.4628e-02,  1.0990e-02, -3.4105e-02,  9.5124e-03,  8.5445e-03,\n",
      "        -1.6453e-02,  5.3161e-03,  2.6167e-02, -3.2698e-03,  2.3324e-02,\n",
      "        -8.4476e-03,  2.9766e-02,  1.3005e-02,  1.1357e-02, -9.4052e-03,\n",
      "         1.8536e-02,  1.9751e-02, -2.1582e-02, -2.5675e-02,  7.7120e-03,\n",
      "         1.1268e-02, -5.7343e-03,  3.4173e-02, -2.2724e-02, -2.5981e-02,\n",
      "        -2.7126e-02,  5.4744e-03,  5.8549e-03, -1.5061e-02,  1.2360e-02,\n",
      "        -2.3837e-02,  4.4748e-03,  1.1448e-02, -3.3212e-02, -2.2510e-02,\n",
      "        -7.7091e-03,  1.5457e-02,  2.6845e-02,  3.0780e-02,  1.0686e-02,\n",
      "        -7.5512e-03,  1.0926e-02,  1.0185e-02,  2.3075e-02, -2.0998e-02,\n",
      "        -2.2419e-03,  3.1275e-02, -2.0239e-02, -3.2225e-02, -7.7744e-03,\n",
      "        -2.8569e-02, -2.3795e-02,  7.9422e-03,  1.5573e-02,  1.6094e-02,\n",
      "         2.3121e-02,  2.2622e-03,  3.3254e-02, -1.7701e-02,  3.3048e-02,\n",
      "         4.7434e-03, -2.3308e-02,  1.2761e-02,  2.2578e-02,  8.1697e-03,\n",
      "         3.2488e-02, -1.4124e-02, -2.8213e-02, -3.3251e-02,  7.1513e-03,\n",
      "         1.2034e-02, -9.3064e-03, -2.5675e-02,  3.2031e-03,  1.7520e-02,\n",
      "         2.0057e-02, -3.4198e-02,  5.2604e-03, -2.3294e-02,  1.0786e-02,\n",
      "         1.9523e-02, -3.3382e-02, -1.9586e-02, -1.7258e-02,  1.1517e-02,\n",
      "         2.7054e-03,  2.5431e-02, -1.0786e-02, -1.1577e-02,  7.5847e-03,\n",
      "         1.3339e-02,  1.2191e-02,  2.3007e-02,  1.7244e-02, -5.1471e-03,\n",
      "         3.1722e-02, -1.1766e-02, -1.9214e-02,  3.4409e-02,  2.5964e-02,\n",
      "        -2.9248e-02, -3.3239e-02, -9.9458e-03, -3.3175e-02,  6.9036e-03,\n",
      "         2.1865e-02,  2.8546e-02, -5.2925e-03,  1.2753e-02, -1.2399e-02,\n",
      "        -1.1003e-02,  8.9565e-04, -3.3262e-02, -7.6586e-03,  3.3485e-02,\n",
      "         6.6119e-03, -3.3369e-02, -1.2091e-02, -2.9916e-02,  9.6755e-03,\n",
      "         1.1942e-02, -1.9451e-02, -1.8380e-03, -2.8469e-02,  2.9745e-02,\n",
      "         7.3945e-03,  1.7750e-02, -8.0584e-03,  2.7539e-02,  2.0589e-02,\n",
      "        -1.8220e-02, -3.4796e-02,  6.6358e-03,  9.1366e-03,  2.6362e-02,\n",
      "        -1.3130e-02, -2.9548e-02,  1.1143e-02,  1.8537e-02,  3.4009e-02,\n",
      "         4.8852e-03, -1.1809e-02, -8.7837e-03, -1.8480e-02, -2.0438e-03,\n",
      "        -2.1678e-02, -3.1268e-02])), ('hidden_layers.1.weight', tensor([[ 0.0190,  0.0162,  0.0154,  ...,  0.0439,  0.0238,  0.0040],\n",
      "        [ 0.0298,  0.0236, -0.0216,  ...,  0.0178,  0.0287,  0.0190],\n",
      "        [-0.0071, -0.0179, -0.0118,  ...,  0.0091, -0.0283, -0.0402],\n",
      "        ...,\n",
      "        [-0.0089,  0.0183,  0.0103,  ...,  0.0418,  0.0383, -0.0394],\n",
      "        [ 0.0243, -0.0208, -0.0064,  ...,  0.0435, -0.0043,  0.0200],\n",
      "        [-0.0013,  0.0100,  0.0407,  ..., -0.0150, -0.0407, -0.0086]])), ('hidden_layers.1.bias', tensor([-0.0178, -0.0406,  0.0163,  0.0306, -0.0336, -0.0122,  0.0362, -0.0070,\n",
      "         0.0429, -0.0400, -0.0132,  0.0384,  0.0302, -0.0295, -0.0130, -0.0254,\n",
      "        -0.0115, -0.0339,  0.0430, -0.0324,  0.0152, -0.0412, -0.0342, -0.0022,\n",
      "        -0.0293,  0.0255,  0.0027, -0.0053,  0.0027, -0.0010,  0.0092,  0.0367,\n",
      "         0.0101,  0.0083, -0.0047,  0.0163,  0.0350,  0.0043, -0.0355,  0.0374,\n",
      "        -0.0147,  0.0114, -0.0239, -0.0016, -0.0425, -0.0400, -0.0150,  0.0120,\n",
      "        -0.0273, -0.0043, -0.0317,  0.0437,  0.0351, -0.0402, -0.0192, -0.0179,\n",
      "         0.0363, -0.0074, -0.0043,  0.0115,  0.0367,  0.0057,  0.0041,  0.0169,\n",
      "        -0.0284, -0.0160,  0.0193, -0.0361, -0.0399,  0.0023, -0.0031,  0.0358,\n",
      "        -0.0081, -0.0380, -0.0314, -0.0036, -0.0040,  0.0145, -0.0005,  0.0080,\n",
      "        -0.0376, -0.0348,  0.0127, -0.0190,  0.0040,  0.0041, -0.0024,  0.0429,\n",
      "        -0.0405, -0.0182, -0.0075, -0.0375, -0.0354, -0.0003,  0.0101, -0.0244,\n",
      "        -0.0152, -0.0070,  0.0341,  0.0185, -0.0171,  0.0245,  0.0139,  0.0398,\n",
      "         0.0442, -0.0361,  0.0056,  0.0436, -0.0218, -0.0322,  0.0345, -0.0097,\n",
      "        -0.0305, -0.0121, -0.0311,  0.0070, -0.0379,  0.0190, -0.0380,  0.0388,\n",
      "         0.0338,  0.0376,  0.0285,  0.0376, -0.0154, -0.0433, -0.0183, -0.0022,\n",
      "         0.0346, -0.0144,  0.0199, -0.0285, -0.0275, -0.0262, -0.0200,  0.0118,\n",
      "        -0.0142, -0.0154, -0.0322,  0.0287,  0.0089,  0.0363, -0.0415, -0.0156,\n",
      "         0.0305,  0.0284,  0.0303,  0.0205, -0.0138,  0.0277,  0.0062,  0.0357,\n",
      "        -0.0120,  0.0248, -0.0023,  0.0116, -0.0393,  0.0223, -0.0244,  0.0052,\n",
      "        -0.0086, -0.0305,  0.0107,  0.0356, -0.0089,  0.0025,  0.0278, -0.0057,\n",
      "         0.0122, -0.0242, -0.0415,  0.0386,  0.0094, -0.0322,  0.0428,  0.0113,\n",
      "         0.0019, -0.0076,  0.0420,  0.0003, -0.0337, -0.0076,  0.0319, -0.0325,\n",
      "        -0.0162,  0.0137, -0.0138,  0.0416,  0.0213,  0.0294, -0.0138,  0.0144,\n",
      "         0.0375, -0.0315, -0.0426, -0.0298, -0.0411,  0.0244, -0.0249, -0.0025,\n",
      "         0.0426,  0.0141,  0.0285, -0.0400,  0.0006, -0.0180,  0.0423, -0.0428,\n",
      "        -0.0388, -0.0208, -0.0212, -0.0191, -0.0139, -0.0013, -0.0432, -0.0179,\n",
      "        -0.0023,  0.0028,  0.0121,  0.0280,  0.0376,  0.0384,  0.0410,  0.0231,\n",
      "         0.0015, -0.0139, -0.0045,  0.0071,  0.0205,  0.0146, -0.0141,  0.0054,\n",
      "         0.0363, -0.0032,  0.0228,  0.0065,  0.0175,  0.0119,  0.0101,  0.0012,\n",
      "         0.0169,  0.0205,  0.0115,  0.0038,  0.0018, -0.0061,  0.0101, -0.0344,\n",
      "         0.0116,  0.0320,  0.0142, -0.0095,  0.0283,  0.0124,  0.0078, -0.0172])), ('hidden_layers.2.weight', tensor([[-0.0493,  0.0602, -0.0208,  ..., -0.0082,  0.0241, -0.0175],\n",
      "        [ 0.0310, -0.0265,  0.0285,  ..., -0.0444,  0.0420,  0.0474],\n",
      "        [-0.0246, -0.0610, -0.0390,  ..., -0.0615, -0.0143, -0.0507],\n",
      "        ...,\n",
      "        [-0.0110,  0.0442,  0.0564,  ...,  0.0169,  0.0473, -0.0275],\n",
      "        [-0.0139, -0.0318, -0.0362,  ..., -0.0225, -0.0255,  0.0317],\n",
      "        [-0.0411,  0.0186, -0.0009,  ...,  0.0543, -0.0085,  0.0215]])), ('hidden_layers.2.bias', tensor([ 0.0027,  0.0535, -0.0124, -0.0091,  0.0225,  0.0404,  0.0289,  0.0420,\n",
      "        -0.0597, -0.0107,  0.0247,  0.0562,  0.0441, -0.0159,  0.0620, -0.0448,\n",
      "         0.0048,  0.0208, -0.0517,  0.0405, -0.0126, -0.0459, -0.0599, -0.0206,\n",
      "        -0.0254,  0.0293, -0.0608, -0.0255, -0.0280,  0.0068, -0.0346,  0.0520,\n",
      "        -0.0136, -0.0579, -0.0526, -0.0208,  0.0319, -0.0119, -0.0570, -0.0227,\n",
      "         0.0255, -0.0537, -0.0109,  0.0224, -0.0367, -0.0377, -0.0127,  0.0155,\n",
      "         0.0225,  0.0132, -0.0427,  0.0289, -0.0120,  0.0513, -0.0336, -0.0453,\n",
      "        -0.0198,  0.0464,  0.0282, -0.0463, -0.0393, -0.0216, -0.0354, -0.0466,\n",
      "         0.0295, -0.0262, -0.0345, -0.0319,  0.0422,  0.0155,  0.0532, -0.0092,\n",
      "         0.0379, -0.0485, -0.0559,  0.0533, -0.0357,  0.0612,  0.0357,  0.0381,\n",
      "        -0.0364,  0.0122, -0.0035, -0.0201,  0.0438, -0.0246, -0.0299,  0.0376,\n",
      "        -0.0356, -0.0507,  0.0433,  0.0295, -0.0335,  0.0295,  0.0073, -0.0392,\n",
      "        -0.0481,  0.0082, -0.0278,  0.0463,  0.0548, -0.0223,  0.0196,  0.0572,\n",
      "        -0.0138,  0.0139, -0.0512, -0.0276, -0.0301, -0.0325,  0.0418, -0.0296,\n",
      "         0.0461, -0.0161, -0.0418,  0.0087, -0.0074, -0.0586, -0.0354, -0.0533,\n",
      "         0.0031, -0.0582,  0.0275, -0.0027, -0.0478, -0.0128,  0.0260, -0.0418])), ('output.weight', tensor([[-0.0856, -0.0111,  0.0422,  ..., -0.0654,  0.0772, -0.0054],\n",
      "        [ 0.0120, -0.0290,  0.0502,  ..., -0.0491, -0.0262,  0.0850],\n",
      "        [-0.0589,  0.0503, -0.0683,  ..., -0.0519, -0.0192,  0.0229],\n",
      "        ...,\n",
      "        [-0.0450, -0.0597, -0.0258,  ..., -0.0042,  0.0730,  0.0778],\n",
      "        [ 0.0671,  0.0382,  0.0026,  ..., -0.0182, -0.0384, -0.0558],\n",
      "        [-0.0757, -0.0159,  0.0802,  ..., -0.0659, -0.0601, -0.0589]])), ('output.bias', tensor([ 0.0001, -0.0868, -0.0847,  0.0356, -0.0327, -0.0376, -0.0620,  0.0151,\n",
      "         0.0838,  0.0427]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
